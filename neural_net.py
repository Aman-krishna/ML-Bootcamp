# -*- coding: utf-8 -*-
"""Neural_Net.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_fvSMoqW07sUIiusj14FWPiqPetJN7Qs
"""

import pandas as pd
import numpy as np

def equation(x,theta,bias):
  z = np.dot(x,theta) + bias
  y_predicted = sigmoid(z)
  return y_predicted

def sigmoid(z):
  return 1/(1+np.exp(-z))

def error(x,theta,bias):
  n = x.shape[0]
  y_predicted = equation(x,theta,bias)
  err = np.dot(-y,np.log(y_predicted)) - np.dot((1-y),np.log(1-y_predicted))
  return err/n

def gradient(x,theta,bias,y):
  y_predicted = equation(x,theta,bias)
  n = x.shape[0]
  grad = np.dot(x.T,y_predicted)
  return grad/n
bias = [] 
def gradient_descent(x,y,bias,learning_rate = 0.05,epochs = 200):
  m,n = x.shape
  theta = np.zeros(n,)   # elements in theta represents number of characteristics 
  for i in range(epochs):
    grad = gradient(x,theta,bias,y)
    theta = theta - learning_rate*grad
  for i in grad:
    bias = bias - learning_rate*i
    bias = bias.append(bias)
  return theta,bias

data = pd.read_csv('sample_data/mnist_train_small.csv')
y = data.iloc[:,0]
x = data.iloc[:,1:]
theta,bias = gradient_descent(x,bias,y,learning_rate=0.05,epochs=200)
y_predicted = equation(x,theta,bias)
y_predicted[y_predicted >= 0.5] = 1
y_predicted[y_predicted < 0.5] = 0

y_predicted

